{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ADM_HW3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "phLVUhTBcCJ2"
      },
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpXqY8lscI-n",
        "outputId": "4b515f91-7aa9-48c4-e69f-6518206bfd64"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2k4jQIPTccQA"
      },
      "source": [
        "\n",
        "1.1 Get the list of animes\n",
        "\n",
        "With the for loop I get the url of all the pages (50 per page except the last one which has less,383 pag). We 'send' a request to the page, to collect the ursl. We read the html and take the href attributes of the tag. The command tqdm is useful because it shows the progress status."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wyke871Pce4T",
        "outputId": "da86a77c-8489-470d-be58-009281006ca4"
      },
      "source": [
        "urls=[]\n",
        "for i in tqdm(range(0,20000,50)):\n",
        "  url='https://myanimelist.net/topanime.php?limit='+str(i)\n",
        "  \n",
        "  soup= BeautifulSoup(requests.get(url).text,'html.parser')\n",
        "\n",
        "  for tag in soup.find_all('tr'):\n",
        "    links=tag.find_all('a')\n",
        "    for l in links:\n",
        "      if type(l.get('id'))== str and len(l.contents[0]) >1:\n",
        "        urls.append(l.get('href'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 400/400 [03:09<00:00,  2.11it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPENB_rjckdf",
        "outputId": "4fb1e08c-fa55-4c8e-c983-01b95b007ed4"
      },
      "source": [
        "len(urls)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19134"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrcYZ-d5clZZ"
      },
      "source": [
        "We need to save it as text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JW66AFdcoVR"
      },
      "source": [
        "with open('urls.txt', 'w', encoding='utf-8') as f: #open the file con write, chiude da solo in automatico, lo trovo in file sample data\n",
        "    for line in urls:\n",
        "        f.write(line)\n",
        "        f.write('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHafTHZAcxA9"
      },
      "source": [
        "#!ls pages/3pag & just a check, this is used to see what there is inside 3pag"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d8hXeEnc0y5"
      },
      "source": [
        "1.2 Crawl animes\n",
        "\n",
        "Create the folders. Put all of them under \"pages\". Each folder has a name that refers to the number of the page from which the links it contains come from."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "id": "qamFnit6cypp",
        "outputId": "400e242e-ec90-4753-ca92-1616e7c3b947"
      },
      "source": [
        "import os\n",
        "for page in tqdm(range(1, 384)):\n",
        "    folder = \"page\"+str(page)\n",
        "    path = \"/content/drive/MyDrive/Anime_folder\"+folder\n",
        "    os.mkdir(path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/383 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileExistsError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-8a8ade5f0812>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mfolder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"page\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/Anime_folder\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: '/content/drive/MyDrive/Anime_folderpage1'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77rxGt7KdAY9"
      },
      "source": [
        "!rm -rf pages\n",
        "   #to delete folders that are not needed."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Obgo8WdIdHBx"
      },
      "source": [
        "Try for the first 150:ok. Try to collect them in group of 3."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kmm9t0lTdH44",
        "outputId": "5cc4989b-ca24-4744-e79f-edf0812e53da"
      },
      "source": [
        "for page in tqdm(range(0, 3)):  # page 1 --> 383\n",
        "    fomglder = \"/content/drive/MyDrive/Anime_folderpage\"+str(page+1)\n",
        "    update_page = 50*page\n",
        "    for i in range(0,50):   # 1 -> 50\n",
        "        url = f'{urls[update_page+i]}'\n",
        "        response = requests.get(url)   \n",
        "        filename = r\"\"+folder+\"/anime_\"+str(update_page+i+1)+\".html\"\n",
        "        with open(filename,'w', encoding='utf-8') as f:\n",
        "            f.write(response.text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [01:42<00:00, 34.12s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNvwpC7KdQE2",
        "outputId": "fb817fef-d73d-4ac7-d726-146172f01b93"
      },
      "source": [
        "for page in tqdm(range(4, 8)):  # page 1 --> 383\n",
        "    folder = \"/content/drive/MyDrive/Anime_folderpage\"+str(page+1)\n",
        "    update_page = 50*page\n",
        "    for i in range(0,50):   # 1 -> 50\n",
        "        url = f'{urls[update_page+i]}'\n",
        "        response = requests.get(url)   \n",
        "        filename = r\"\"+folder+\"/anime_\"+str(update_page+i+1)+\".html\"\n",
        "        with open(filename,'w', encoding='utf-8') as f:\n",
        "            f.write(response.text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [00:06<00:00,  1.68s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnLMDl3_dS2h"
      },
      "source": [
        "\n",
        "1.3 Parsing downloaded pages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3OBfiGMdUoE"
      },
      "source": [
        "'''col = ['animeTitle', 'animeType', 'animeNumEpisode', 'releaseDate', 'endDate', 'animeNumMembers', 'animeScore', \\\n",
        "           'animeUsers', 'animeRank', 'animePopularity', 'animeDescription', 'animeRelated', 'animeCharacters', \\\n",
        "           'animeVoices', 'animeStaff']\n",
        "           '''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDkKijVzdWuG"
      },
      "source": [
        "def AnimeTitle(html):\n",
        "  with open(html,'r') as f:\n",
        "    soup= BeautifulSoup(f)\n",
        "    animeTitle=soup.find(\"h1\", attrs = {\"class\": \"title-name h1_bold_none\"}).string\n",
        "    return(animeTitle)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "yzjBHOAQdYqW",
        "outputId": "d34ad50f-b60b-4738-b8ec-7677d70daaf2"
      },
      "source": [
        "AnimeTitle('/content/drive/MyDrive/Anime_folderpage1/anime_10.html')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Gintama: The Final'"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRARLmBKda72",
        "outputId": "4b2cf6cc-f825-4bf9-af2a-7cbdeeca8f81"
      },
      "source": [
        "animeScore('/content/drive/MyDrive/Anime_folderpage1/anime_13.html')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8.97"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iIl-gIPddYc"
      },
      "source": [
        "def animeScore(html):\n",
        "  with open(html,'r') as f:\n",
        "    soup= BeautifulSoup(f)\n",
        "    for i in range(10):\n",
        "      score=soup.find_all('div' ,attrs = {\"class\": \"score-label score-\"+str(i)})\n",
        "      for j in score:\n",
        "        return float(j.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBdGz04hdgm0"
      },
      "source": [
        "def AnimeRank(html):\n",
        "  with open(html,'r') as f:\n",
        "    soup= BeautifulSoup(f)\n",
        "    rank=int(str(soup.find(class_=\"numbers ranked\").text).split('#')[-1])  \n",
        "    return(rank)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQdc55DKdhIm",
        "outputId": "d484a582-a846-4029-f046-30873683b61e"
      },
      "source": [
        "AnimeRank('/content/drive/MyDrive/Anime_folderpage1/anime_13.html')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SP2nhzYydjT3"
      },
      "source": [
        "\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVN4OBmudk8A"
      },
      "source": [
        "def animeNumberofepisode(html_string):\n",
        "  with open(html_string, 'r') as f:\n",
        "    soup = BeautifulSoup(f, 'html.parser')\n",
        "    \n",
        "    A=soup.find(text=re.compile('Episodes:')).parent.parent.text.split()[-1]      \n",
        "    print(A)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZd8AYFUdmzY",
        "outputId": "a61129c9-a26c-4ce8-be6b-e183f33c2441"
      },
      "source": [
        "animeNumberofepisode('/content/drive/MyDrive/Anime_folderpage1/anime_1.html')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEKSDJRLdoja"
      },
      "source": [
        "def AnimePopularity(html):\n",
        "  with open(html,'r') as f:\n",
        "    soup= BeautifulSoup(f)\n",
        "    rank=int(str(soup.find(class_=\"numbers popularity\").text).split('#')[-1])  \n",
        "    return(rank)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEU2lgAQdqWK",
        "outputId": "f098c587-6638-428a-851c-335addbded6b"
      },
      "source": [
        "AnimePopularity('/content/drive/MyDrive/Anime_folderpage1/anime_13.html')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3J-GnvnLdvRl"
      },
      "source": [
        "def animeDescription(html_string): #I think that this function works well.\n",
        "  with open(html_string, 'r') as f:\n",
        "    soup = BeautifulSoup(f, 'html.parser')\n",
        "    A=soup.find(itemprop=\"description\")\n",
        "    return A.get_text()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ebntLTGadwBM",
        "outputId": "9e4102bc-d2a7-4351-a607-9efd2babd0fc"
      },
      "source": [
        "animeDescription('/content/drive/MyDrive/Anime_folderpage1/anime_10.html')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'New Gintama movie.'"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDPvA38ddyDI"
      },
      "source": [
        "def animeUsers(html_string): #I think that this function works well.\n",
        "  with open(html_string, 'r') as f:\n",
        "    soup = BeautifulSoup(f, 'html.parser')\n",
        "    A=soup.find(itemprop=\"ratingCount\")\n",
        "    return int(A.get_text())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s84ZGFBcd0OV",
        "outputId": "f019abe7-1edc-450a-9b8c-f7320f447a92"
      },
      "source": [
        "animeUsers('/content/drive/MyDrive/Anime_folderpage1/anime_13.html')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1227199"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j588B0p5d19h"
      },
      "source": [
        "def animeType(html_string):\n",
        "  with open(html_string, 'r') as f:\n",
        "    soup = BeautifulSoup(f, 'html.parser')\n",
        "    \n",
        "    A=soup.find(text=re.compile('Type:')).parent.parent.text.split()[-1]      \n",
        "    print(A)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0lIsIQZd3kM",
        "outputId": "4bc9c634-86a6-4fa5-a8b0-ee60a90e9b7b"
      },
      "source": [
        "animeType('/content/drive/MyDrive/Anime_folderpage1/anime_11.html')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TV\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygFaYEYGd5h_"
      },
      "source": [
        "def fai_date(a: list):\n",
        "    \n",
        "    string = ''\n",
        "    for i in a:\n",
        "        string += i\n",
        "        if i != a[-1]:\n",
        "            string += ' '\n",
        "    string = string.replace(',', '')\n",
        "    date = datetime.strptime(string, '%b %d %Y')\n",
        "    return date"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nC390Eyxd7Vd"
      },
      "source": [
        "def animeRelDate(html_string):\n",
        "  with open(html_string, 'r') as f:\n",
        "    soup = BeautifulSoup(f, 'html.parser')\n",
        "    \n",
        "    A= fai_date(soup.find(text=re.compile('Aired:'), class_=\"dark_text\").parent.text.split()[1:4]  )   \n",
        "    print(A)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vOCazOCoSEd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "KVHmzmwSd96H",
        "outputId": "f93bd835-1078-42b8-b195-b250aab2262a"
      },
      "source": [
        "animeRelDate('/content/drive/MyDrive/Anime_folderpage1/anime_11.html')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-01d581d43cdc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0manimeRelDate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Anime_folderpage1/anime_11.html'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-30-2b03489d9989>\u001b[0m in \u001b[0;36manimeRelDate\u001b[0;34m(html_string)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mA\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mfai_date\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Aired:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"dark_text\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-ffb516593c7e>\u001b[0m in \u001b[0;36mfai_date\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mstring\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mstring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mdate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'%b %d %Y'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'datetime' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlXgPtdCeAu6"
      },
      "source": [
        "def animeEndDate(html_string):\n",
        "  with open(html_string, 'r') as f:\n",
        "    soup = BeautifulSoup(f, 'html.parser')\n",
        "    \n",
        "    A= fai_date(soup.find(text=re.compile('Aired:'), class_=\"dark_text\").parent.text.split()[5:8]   )  \n",
        "    print(A)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "-UvgCqgmeCoE",
        "outputId": "c62c7aad-1d31-4fdb-ab89-0ef491a5e857"
      },
      "source": [
        "animeEndDate('/content/drive/MyDrive/Anime_folderpage1/anime_11.html')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-5fd371d3611b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0manimeEndDate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Anime_folderpage1/anime_11.html'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-31-4b07c43d22fe>\u001b[0m in \u001b[0;36manimeEndDate\u001b[0;34m(html_string)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mA\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mfai_date\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Aired:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"dark_text\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m   \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-ffb516593c7e>\u001b[0m in \u001b[0;36mfai_date\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mstring\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mstring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mdate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'%b %d %Y'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'datetime' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsGyJuTXeDPj"
      },
      "source": [
        "def animeRelated(html_string):  \n",
        "  animeRelated = []\n",
        "  with open(html_string, 'r') as f:\n",
        "    soup = BeautifulSoup(f, 'html.parser')\n",
        "    related = soup.find_all(\"table\", {\"class\":\"anime_detail_related_anime\"})\n",
        "    for i in related:\n",
        "      links = i.find_all('a')\n",
        "      for link in links:        \n",
        "          animeRelated.append(f'{link.contents[0]}')\n",
        "\n",
        "  return animeRelated"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVrVEeKieFCa",
        "outputId": "b48d972f-c0d5-4ff0-b2cd-e03940c16441"
      },
      "source": [
        "animeRelated('/content/drive/MyDrive/Anime_folderpage1/anime_11.html')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Gintama', 'Gintama°', 'Gintama.: Porori-hen']"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yX4JGeZMeH0Z"
      },
      "source": [
        "def animeCharacter(html_string):\n",
        "  personaggi=[]\n",
        "  with open(html_string, 'r') as f:\n",
        "    soup = BeautifulSoup(f, 'html.parser')\n",
        "    \n",
        "    A= soup.find_all(class_=\"h3_characters_voice_actors\")    \n",
        "    for a in A:\n",
        "      personaggi.append(a.text)\n",
        "    return(personaggi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzqCeBOgeJZt",
        "outputId": "2d2fa976-fd44-41ce-933f-78a7c067f35d"
      },
      "source": [
        "animeCharacter('/content/drive/MyDrive/Anime_folderpage1/anime_11.html')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Sakata, Gintoki',\n",
              " 'Kagura',\n",
              " 'Katsura, Kotarou',\n",
              " 'Takasugi, Shinsuke',\n",
              " 'Shimura, Shinpachi',\n",
              " 'Kamui',\n",
              " 'Elizabeth',\n",
              " 'Imai, Nobume',\n",
              " 'Sadaharu',\n",
              " 'Sakamoto, Tatsuma']"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lv0_M-g3eMBv"
      },
      "source": [
        "def animeVoices(html_string):\n",
        "  personaggi=[]\n",
        "  with open(html_string, 'r') as f:\n",
        "    soup = BeautifulSoup(f, 'html.parser')\n",
        "    \n",
        "    A= soup.find_all(class_=\"va-t ar pl4 pr4\")   \n",
        "    for a in A:\n",
        "      personaggi.append(a.text)\n",
        "\n",
        "      \n",
        "    return(personaggi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qqk2b5eyePYo",
        "outputId": "65e76719-0259-442d-c8ae-c35654fcb31f"
      },
      "source": [
        "animeVoices('/content/drive/MyDrive/Anime_folderpage1/anime_11.html')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\nSugita, Tomokazu\\nJapanese\\n',\n",
              " '\\nKugimiya, Rie\\nJapanese\\n',\n",
              " '\\nIshida, Akira\\nJapanese\\n',\n",
              " '\\nKoyasu, Takehito\\nJapanese\\n',\n",
              " '\\nSakaguchi, Daisuke\\nJapanese\\n',\n",
              " '\\nHino, Satoshi\\nJapanese\\n',\n",
              " '\\nHirano, Aya\\nJapanese\\n',\n",
              " '\\nTakahashi, Mikako\\nJapanese\\n',\n",
              " '\\nMiki, Shinichiro\\nJapanese\\n']"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WpR6mRueRqv"
      },
      "source": [
        "def createstaff(ll):\n",
        "    \"\"\"\n",
        "    puts all relevant text in a list after cleaning it up a little\n",
        "    \"\"\"\n",
        "    l = []\n",
        "    for i in ll:\n",
        "        if i['href'].startswith('https://myanimelist.net/people/') and i['href'] not in l:\n",
        "            if i.text != '\\n\\n':\n",
        "                j = str(i.text).replace('\\n', '')\n",
        "                j = re.sub(' +', ' ', j)\n",
        "            if j[0] == ' ':\n",
        "                j = j[1:]\n",
        "            if j[-1] == ' ':\n",
        "                j = j[:-1]\n",
        "            l.append(j)\n",
        "    return l      \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def animeStaff(html_string):\n",
        "   with open(html_string, 'r') as f:\n",
        "    soup = BeautifulSoup(f, 'html.parser')\n",
        "    \n",
        "    A=  createstaff(soup.find_all('a', href=True))     \n",
        "    print(A)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWn7XsfveTfD",
        "outputId": "4f54cf8c-4e1c-46cc-f90a-a537286b9513"
      },
      "source": [
        "animeStaff('/content/drive/MyDrive/Anime_folderpage1/anime_11.html')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Sugita, Tomokazu', 'Sugita, Tomokazu', 'Kugimiya, Rie', 'Kugimiya, Rie', 'Ishida, Akira', 'Ishida, Akira', 'Koyasu, Takehito', 'Koyasu, Takehito', 'Sakaguchi, Daisuke', 'Sakaguchi, Daisuke', 'Hino, Satoshi', 'Hino, Satoshi', 'Hirano, Aya', 'Hirano, Aya', 'Takahashi, Mikako', 'Takahashi, Mikako', 'Miki, Shinichiro', 'Miki, Shinichiro', 'Miki, Shinichiro', 'Fujita, Youichi', 'Fujita, Youichi', 'Miyawaki, Chizuru', 'Miyawaki, Chizuru', 'Takamatsu, Shinji', 'Takamatsu, Shinji', 'Kobayashi, Katsuyoshi']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVnyTX2Ud4AP"
      },
      "source": [
        "col = ['animeTitle', 'animeType', 'animeNumEpisode', 'releaseDate', 'endDate', 'animeNumMembers', 'animeScore', 'animeUsers', 'animeRank', 'animePopularity', 'animeDescription', 'animeRelated', 'animeCharacters', 'animeVoices', 'animeStaff']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "GcfeYIPnm3VH",
        "outputId": "f8bc7d6d-a618-4e41-879e-d6a280b772ae"
      },
      "source": [
        "records=[]\n",
        "for soup in urls:\n",
        "  animeTitle=soup.find(\"h1\", attrs = {\"class\": \"title-name h1_bold_none\"}).string\n",
        "  records.append((animeTitle))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-80d8ed3b59a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msoup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0murls\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0manimeTitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"h1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"class\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"title-name h1_bold_none\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mrecords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manimeTitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: find() takes no keyword arguments"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drkgO2dTjwYD"
      },
      "source": [
        "df=pd.DataFrame(\n",
        "    {\n",
        "        'animeTitle':[AnimeTitle], \n",
        "        'animeType':[animeType], \n",
        "        'animeNumEpisode':[], \n",
        "        'releaseDate':[], \n",
        "        'endDate':[], \n",
        "        'animeNumMembers':[], \n",
        "        'animeScore':[], \n",
        "        'animeUsers':[], \n",
        "        'animeRank':[], \n",
        "        'animePopularity':[], \n",
        "        'animeDescription':[], \n",
        "        'animeRelated':[], \n",
        "        'animeCharacters':[], \n",
        "        'animeVoices':[], \n",
        "        'animeStaff':[]\n",
        "    }\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYJiSfz3daGX"
      },
      "source": [
        "from datamatrix import DataMatrix\n",
        "dm=DataMatrix(length=len(col))\n",
        "dm.title=col()\n",
        "dm.info="
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMp-naKTOPxn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uP4iA-bK3m9v"
      },
      "source": [
        "2.1. Conjunctive query\n",
        "For the first version of the search engine, we narrow our interest on the Synopsis of each anime. It means that you will evaluate queries only with respect to the anime's description."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0qpsez33oT3"
      },
      "source": [
        "2.1.1) Create your index!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3U6KjTqS39_-"
      },
      "source": [
        "Create a file named vocabulary, in the format you prefer, that maps each word to an integer (term_id)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-mWP2AHyVGh"
      },
      "source": [
        "from tqdm import tqdm\n",
        "from natsort import natsorted\n",
        "import os\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeKt7Nj-3sJ9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "outputId": "b846b65a-ab92-4483-ceb8-7f8ee343575b"
      },
      "source": [
        "documents = []\n",
        "\n",
        "folder=r\"./tsv_anime/\"\n",
        "for anime in tqdm(natsorted(os.listdir(folder))):\n",
        "  df=pd.read_csv(folder+anime, sep=\"\\t\")\n",
        "  documents.append(df[\"animeDescription\"][0])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-3ac524d3b1ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfolder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mr\"./tsv_anime/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0manime\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnatsorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0manime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mdocuments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"animeDescription\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './tsv_anime/'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqPzpBoZqioL"
      },
      "source": [
        "documents[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iig2ho_qql-S"
      },
      "source": [
        "#core libraries\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import string\n",
        "import re\n",
        "\n",
        "#functions.py\n",
        "import functions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WiKYi7Irehm"
      },
      "source": [
        "contractions={\n",
        "    \"ain't\": \"am not\",\n",
        "    \"aren't\": \"are not\",\n",
        "    \"can't\": \"cannot\",\n",
        "    \"can't've\": \"cannot have\",\n",
        "    \"'cause\": \"bacause\",\n",
        "    \"could've\": \"could have\",\n",
        "    \"couldn't\": \"could not\",\n",
        "    \"didn't\": \"did not\",\n",
        "    \"doesn't\": \"does not\",\n",
        "    /////////////////////\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAufGsm0sdT4"
      },
      "source": [
        "contractions2={\n",
        "    \"min\": \"minute\",\n",
        "    \"sec\": \"second\"\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrfAA1xastxs"
      },
      "source": [
        "def pre_processinf(documents):\n",
        "  stop=stopwords.words(\"english\")\n",
        "  lmtzr=WordNetLemmatizer()\n",
        "  remove=[\"Written\", \"MAL\", \"Rewrite\"]+[\"'s\"]+[\"\"]\n",
        "  ps=PorterStemmer()\n",
        "\n",
        "  #removing contraction+normalization\n",
        "  document_tmp=fuctions.replace_words(documents.lower(), contractions)\n",
        "  document_tmp=\" \".join(re.split(r\"([0-9]+)([a-z]+)\", document_tmp))\n",
        "  document_tmp=functions.replace_words(document_tmp, contractions2)\n",
        "  document_tmp=re.sub(r\"[{}\\-''\"\"]\".format(string.punctuation),\" \",document_tmp)\n",
        "  document_tmp=word_tokenize(document_tmp)\n",
        "  document_tmp=[word for word in document_tmp if word not in stop]\n",
        "  document_tmp=[re.sub(r\"[^a-zA-Z0-9]\",\"\", word).strip() for word in document_tmp]\n",
        "  document_tmp=[word for word in document_tmp if word not in remove]\n",
        "  document_tmp=[lmtzr.lemmatizer(word) for word in document_tmp]\n",
        "  document_tmp=[ps.stem(word) for word in document_tmp]\n",
        "\n",
        "  return document_tmp "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWSjzwweDteE"
      },
      "source": [
        "#cleaning the documents\n",
        "documents_clean=[]\n",
        "for d in documents:\n",
        "  documents_clean.append(pre_processing(d))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBiBGxf7EBeU"
      },
      "source": [
        "documents_clean[0][:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "au6RxSVeEGU4"
      },
      "source": [
        "#creating vocabulary\n",
        "#core libraries\n",
        "import itertools\n",
        "import umpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jqu1JRLMETI8"
      },
      "source": [
        "#the list of all unique words\n",
        "word_list-list(set(list(itertools.chain.form_iterable(documents_clean))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-gxhMHlEwjP"
      },
      "source": [
        "vocabulary=dict(zip(word_list, range(len(word_list))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTdH9slBFEVh"
      },
      "source": [
        "len(vocabulary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2zC5udFFHzh"
      },
      "source": [
        "#view the vocabulary\n",
        "count=0\n",
        "for key, mapped_int in vocabulary.items():\n",
        "  count+=1\n",
        "  print(key,\"-->\", mapped_int)\n",
        "  if count==10: break\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ii0DOUMFd6y"
      },
      "source": [
        "import json\n",
        "file=open(\"vocabulary.json\", \"w\", encoding=\"utf-8\")\n",
        "json.dump(vocabulary, file)\n",
        "file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5aESTFd2EWQn"
      },
      "source": [
        "#import the saved vocabularry\n",
        "with open(\"vocabulary.json\") as f:\n",
        "  vocabulary=json.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jFgs0XdGCiZ"
      },
      "source": [
        "#2.1.CONJUCTIVE QUERY"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c427YiebGPEx"
      },
      "source": [
        "def word_to_int(document, vocabulary):\n",
        "  int_doc=np.zeros(len(document), dtype=np.int64)\n",
        "  for i, word in enumerate(document):\n",
        "    int_doc[i]=vocabulary[word]\n",
        "\n",
        "  return np.sort(int_doc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mAjuHkhG3pd"
      },
      "source": [
        "documents_mapped=[]\n",
        "for d in documents_clean:\n",
        "  documents_mapped.append(word_to_int(d, vocabulary))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtqZvR48HJIK"
      },
      "source": [
        "documents_mapped[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9HoyzkbHQGw"
      },
      "source": [
        "from collections import defaultdict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-zqD4QVHXpU"
      },
      "source": [
        "inverted_index=defaultdict(list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjBArctOHhJz"
      },
      "source": [
        "for i,d in enumerate(documents_mapped):\n",
        "  for word in set(d):\n",
        "    Inverted_index[str(word)].append(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FODEgXUgIAub"
      },
      "source": [
        "count=0\n",
        "for key, lis in Inverted_index.items():\n",
        "  count+=1\n",
        "  print(\"word: \", key,\"-->\",\"documents: \", lis)\n",
        "  if count==1: break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nS3nnXLrIg39"
      },
      "source": [
        "import json\n",
        "\n",
        "file=open(\"Inverted_index_v1.json\",\"w\", encoding=\"utf-8\")\n",
        "json.dump(Inverted_index, file)\n",
        "file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cBmc7bTI94_"
      },
      "source": [
        "with open(\"Inverted_index_v1.json\" ) as f:\n",
        "  Inverted_index=json.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzJLpzZQJNNL"
      },
      "source": [
        "def search_engine_v1(query_text):\n",
        "  query_clean=pre_processing(query_text)\n",
        "  query_int=word_to_int(query_clean, vocabulary)\n",
        "\n",
        "  index=[]\n",
        "  for query in query_int:\n",
        "    index.append(set(Inverted_index[str(query)]))\n",
        "  index=list(index[0].intersection(*index))\n",
        "\n",
        "  with open(\"./anime_url.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    lines=f.readlines()\n",
        "\n",
        "  anime_path=[]\n",
        "  url=[]\n",
        "\n",
        "  for idx in index:\n",
        "    name=\"/anime_\"+str(idx+1)+\".tsv\"\n",
        "    anime_path.append(name)\n",
        "    url.append(lines[idx])\n",
        "  \n",
        "  animes_df=[]\n",
        "  folder=r\"./tsv_anime/\"\n",
        "  cols=[\"animeTitle\", \"animeDescription\"]\n",
        "  for i,anime_tsv in enumerate(anime_path):\n",
        "    df=pd.read_csv(folder+anime_tsv, sep=\"\\t\", usecols=cols)\n",
        "    df[\"animeURL\"]=url[i]\n",
        "    animes_df.append(df)\n",
        "  \n",
        "  frame=pd.concat(animes_df, ignore_index=True)\n",
        "  display(frame)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CS3blYPkL5x7"
      },
      "source": [
        "#input query\n",
        "query_text=input(\"Insert the query: \")\n",
        "\n",
        "search_engine_v1(query_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6heOrO0RMJ3i"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZBB17f-MNvq"
      },
      "source": [
        "### 2.2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBKjz3HiMSP9"
      },
      "source": [
        "n=len(documents)\n",
        "n_words=len(vocabulary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nO9hzOdYMc1_"
      },
      "source": [
        "def word_to_int2(document, vocabulary):\n",
        "  int_doc=np.zeros(len(vocabulary), dtype=np.int64)\n",
        "\n",
        "  for word in document:\n",
        "    int_doc[vocabulary[word]]+=1\n",
        "  return int_doc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-e-LBZy9NO2b"
      },
      "source": [
        "tf=[]\n",
        "for d in documents_clean:\n",
        "  tf.append(word_to_int2(d, vocabulary))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0zcEcxoNgaS"
      },
      "source": [
        "tf[0][tf[0]>0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxpsoqq3NnMm"
      },
      "source": [
        "nj=np.zeros(n_words, dtype=np.float64)\n",
        "\n",
        "for d in documents_mapped:\n",
        "  for word in set(d):\n",
        "    nj[word]+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dC3bG0abN_sq"
      },
      "source": [
        "nj"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efmuekx_OBJ_"
      },
      "source": [
        "idf=np.zeros(n_words, dtype=np.float64)\n",
        "idf=np.log(n/nj)/np.log(n)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sM8fZ1ieOQlo"
      },
      "source": [
        "tfIdf=np.multiply(tf, idf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01RSI1aEOa1H"
      },
      "source": [
        "np.shape(tfIdf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUjbzekpOe5p"
      },
      "source": [
        "from collections import defaultdict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pr3vV_SsOm7v"
      },
      "source": [
        "Inverted_indexv2=defaultdict(list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8LkpsldOuSG"
      },
      "source": [
        "for i,d in enumerate(documents_mapped):\n",
        "  for word in set(d):\n",
        "    Inverted_indexv2[str(word)].append((i,tfIdf[i, int(word)]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fA2BirvcPJjw"
      },
      "source": [
        "count=0\n",
        "for key, lis in Inverted_indexv2.items():\n",
        "  count+=1\n",
        "  print(key, \"-->\",lis)\n",
        "  if count==1: break\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AK2fmNzPagn"
      },
      "source": [
        "import json\n",
        "\n",
        "file=open(\"inverted_index_v2.json\", \"w\", encoding=\"utf-8\")\n",
        "json.dump(Inverted_indexv2, file)\n",
        "file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZhWNGNOPw7B"
      },
      "source": [
        "with open(\"Inverted_index_v2.json\") as f:\n",
        "  Inverted_indexv2 = json.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cf0KhyB_P9E9"
      },
      "source": [
        "import heapq "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmx0NjnaQBTW"
      },
      "source": [
        "def search_engine2(query, k, tfIdf):\n",
        "  query_clean=pre_processing(query)\n",
        "  query_int=word_to_int2(query_clean, vocabulary)\n",
        "\n",
        "  match_list=[]\n",
        "  lenMatch=[]\n",
        "  max_lenMatch=(0,-1)\n",
        "  for i,query in enumerate(np.where(query_int>0)[0]):\n",
        "    lis=Inverted_indexv2[str(query)]\n",
        "    match_list.append(lis)\n",
        "    tmplis, tmplen = i, len(lis)\n",
        "    if tmplen>max_lenMatch[1]:\n",
        "      max_lenMatch=(tmplis, tmplen)\n",
        "    lenMatch.append(len(lis))\n",
        "\n",
        "  enough, match_list=functions.intersection_all(k, match_list)\n",
        "  m=len(query_clean)\n",
        "  if enough:\n",
        "    scores=functions.scoresK(match_list, tfIdf, m, query_int)\n",
        "    topscore, topk=fucntions.find_topK(k, scores)\n",
        "  else:\n",
        "    scores=functions.scoreALL(match_list, tfIdf, m, lenMatch)\n",
        "    k=len(scores)\n",
        "    topscore, topk=functions.find_topK(k, scores)\n",
        "  with open(\"./anime_url.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    lines=f.readlines()\n",
        "\n",
        "  anime_path=[]\n",
        "  url=[]\n",
        "\n",
        "  for idx in topk:\n",
        "    name = \"/anime_\"+str(idx+1)+\".tsv\"\n",
        "    anime_path.append(name)\n",
        "    url.append(lines[idx])\n",
        "\n",
        "  animes_df=[]\n",
        "  folder=r\"./tsv_anime/\"\n",
        "  cols=[\"animeTitle\", \"animeDescription\"]\n",
        "  for i, anime_tsv in enumerate(anime_path):\n",
        "    df=pd.read_csv(folder+anime_tsv, sep=\"\\t\", usecols=cols)\n",
        "\n",
        "    df[\"animeURL\"]=url[i]\n",
        "    df[\"animeScores\"]=topscore[i]\n",
        "    animes_df.append(df)\n",
        "\n",
        "  frame=pd.concat(animes_df, ignore_index=True)\n",
        "  display(frame)\n",
        "  return\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXUg55BjUFSq"
      },
      "source": [
        "### Input and searching"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rowVEIwJULW5"
      },
      "source": [
        "query=input(\"Insert the query: \")\n",
        "k=int(input(\"Insert k: \"))\n",
        "\n",
        "search_engine2(query, k, tfIdf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1WfXzx4UcA4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}